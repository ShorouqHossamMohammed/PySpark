{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c325fee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7af762d07490>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c39c244",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame([('Ahmed',25),('Mohamed',32),('Ahmed',15),('Mohamed',11)],schema=['Name','age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224dfc97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: bigint]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64164b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5c02850",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupBy('Name').avg('age')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "291271d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, avg(age): double]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc6dd73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- avg(age): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6057d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: bigint]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76f8bc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|   Name|avg(age)|\n",
      "+-------+--------+\n",
      "|  Ahmed|    20.0|\n",
      "|Mohamed|    21.5|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4ba9df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, age: bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b9da583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a5da49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.groupBy('Name').agg(avg('age').alias('AvgAge'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc6ff276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- AvgAge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df3.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac8a3a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df3.write.parquet('MansParq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8cd5113",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8a518d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write.csv('MansParqCSV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6cdf2376",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.write.json('MansParqJson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19402934",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlights = spark.read.csv('csv',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "52f52800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFlights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3dc857de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlights = spark.read.csv('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e38bd5a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFlights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4d5960c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlights = spark.read.csv('csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dee52950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFlights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "660682de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlights = spark.read.csv('csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "55ab03bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFlights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6fde5765",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlights = spark.read.csv('csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "256f5ab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFlights.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "842ecc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCSV = spark.read.csv('MansParqCSV',inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa9023e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCSV.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5057fa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfparq = spark.read.parquet('MansParq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2c45dc1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- AvgAge: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfparq.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "107ebdb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    ".option('inferschema','true')\\\n",
    ".option('header','true')\\\n",
    ".load('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb2c1225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b03a2c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', IntegerType(), True)])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de3317d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType,StructField,StringType, IntegerType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2854b554",
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema = StructType([StructField('DestinationCountry',StringType(),True),\n",
    "                      StructField('OriginCountry',StringType(),True),\n",
    "                      StructField('FlightCounts',IntegerType(),True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "82fffcf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "myschema = 'DestinationCountry STRING, OriginCountry STRING, FlightCounts INTEGER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "77609e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    ".option('header','true')\\\n",
    ".schema(myschema)\\\n",
    ".load('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "78d596aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DestinationCountry: string (nullable = true)\n",
      " |-- OriginCountry: string (nullable = true)\n",
      " |-- FlightCounts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b325686a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlightsSample = spark.read.csv('csv',inferSchema=True,header=True,samplingRatio=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "23a5dcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = dfFlightsSample.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2f0ab4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('DEST_COUNTRY_NAME', StringType(), True), StructField('ORIGIN_COUNTRY_NAME', StringType(), True), StructField('count', IntegerType(), True)])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "74a8787a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------------+\n",
      "|  DestinationCountry|   OriginCountry|FlightCounts|\n",
      "+--------------------+----------------+------------+\n",
      "|       United States|         Romania|           1|\n",
      "|       United States|         Ireland|         264|\n",
      "|       United States|           India|          69|\n",
      "|               Egypt|   United States|          24|\n",
      "|   Equatorial Guinea|   United States|           1|\n",
      "|       United States|       Singapore|          25|\n",
      "|       United States|         Grenada|          54|\n",
      "|          Costa Rica|   United States|         477|\n",
      "|             Senegal|   United States|          29|\n",
      "|       United States|Marshall Islands|          44|\n",
      "|              Guyana|   United States|          17|\n",
      "|       United States|    Sint Maarten|          53|\n",
      "|               Malta|   United States|           1|\n",
      "|             Bolivia|   United States|          46|\n",
      "|            Anguilla|   United States|          21|\n",
      "|Turks and Caicos ...|   United States|         136|\n",
      "|       United States|     Afghanistan|           2|\n",
      "|Saint Vincent and...|   United States|           1|\n",
      "|               Italy|   United States|         390|\n",
      "|       United States|          Russia|         156|\n",
      "+--------------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/29 11:00:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2010-summary.csv\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "38e3c2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    ".schema(myschema)\\\n",
    ".load('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d95f77d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------+\n",
      "|  DestinationCountry|      OriginCountry|FlightCounts|\n",
      "+--------------------+-------------------+------------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|        NULL|\n",
      "|       United States|            Romania|           1|\n",
      "|       United States|            Ireland|         264|\n",
      "|       United States|              India|          69|\n",
      "|               Egypt|      United States|          24|\n",
      "|   Equatorial Guinea|      United States|           1|\n",
      "|       United States|          Singapore|          25|\n",
      "|       United States|            Grenada|          54|\n",
      "|          Costa Rica|      United States|         477|\n",
      "|             Senegal|      United States|          29|\n",
      "|       United States|   Marshall Islands|          44|\n",
      "|              Guyana|      United States|          17|\n",
      "|       United States|       Sint Maarten|          53|\n",
      "|               Malta|      United States|           1|\n",
      "|             Bolivia|      United States|          46|\n",
      "|            Anguilla|      United States|          21|\n",
      "|Turks and Caicos ...|      United States|         136|\n",
      "|       United States|        Afghanistan|           2|\n",
      "|Saint Vincent and...|      United States|           1|\n",
      "|               Italy|      United States|         390|\n",
      "+--------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c69bc9ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DestinationCountry: string (nullable = true)\n",
      " |-- OriginCountry: string (nullable = true)\n",
      " |-- FlightCounts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "04a6166b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    ".schema(myschema)\\\n",
    ".option('mode','PERMISSIVE')\\\n",
    ".load('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "62b2c2ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------------+\n",
      "|  DestinationCountry|      OriginCountry|FlightCounts|\n",
      "+--------------------+-------------------+------------+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|        NULL|\n",
      "|       United States|            Romania|           1|\n",
      "|       United States|            Ireland|         264|\n",
      "|       United States|              India|          69|\n",
      "|               Egypt|      United States|          24|\n",
      "|   Equatorial Guinea|      United States|           1|\n",
      "|       United States|          Singapore|          25|\n",
      "|       United States|            Grenada|          54|\n",
      "|          Costa Rica|      United States|         477|\n",
      "|             Senegal|      United States|          29|\n",
      "|       United States|   Marshall Islands|          44|\n",
      "|              Guyana|      United States|          17|\n",
      "|       United States|       Sint Maarten|          53|\n",
      "|               Malta|      United States|           1|\n",
      "|             Bolivia|      United States|          46|\n",
      "|            Anguilla|      United States|          21|\n",
      "|Turks and Caicos ...|      United States|         136|\n",
      "|       United States|        Afghanistan|           2|\n",
      "|Saint Vincent and...|      United States|           1|\n",
      "|               Italy|      United States|         390|\n",
      "+--------------------+-------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "21ef8755",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    ".schema(myschema)\\\n",
    ".option('mode','dropmalformed')\\\n",
    ".load('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "18593b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------------+------------+\n",
      "|DestinationCountry              |OriginCountry   |FlightCounts|\n",
      "+--------------------------------+----------------+------------+\n",
      "|United States                   |Romania         |1           |\n",
      "|United States                   |Ireland         |264         |\n",
      "|United States                   |India           |69          |\n",
      "|Egypt                           |United States   |24          |\n",
      "|Equatorial Guinea               |United States   |1           |\n",
      "|United States                   |Singapore       |25          |\n",
      "|United States                   |Grenada         |54          |\n",
      "|Costa Rica                      |United States   |477         |\n",
      "|Senegal                         |United States   |29          |\n",
      "|United States                   |Marshall Islands|44          |\n",
      "|Guyana                          |United States   |17          |\n",
      "|United States                   |Sint Maarten    |53          |\n",
      "|Malta                           |United States   |1           |\n",
      "|Bolivia                         |United States   |46          |\n",
      "|Anguilla                        |United States   |21          |\n",
      "|Turks and Caicos Islands        |United States   |136         |\n",
      "|United States                   |Afghanistan     |2           |\n",
      "|Saint Vincent and the Grenadines|United States   |1           |\n",
      "|Italy                           |United States   |390         |\n",
      "|United States                   |Russia          |156         |\n",
      "+--------------------------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "382546d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    ".schema(myschema)\\\n",
    ".option('mode','failfast')\\\n",
    ".load('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "0da0b472",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/29 11:19:07 ERROR Executor: Exception in task 0.0 in stage 44.0 (TID 76)\n",
      "org.apache.spark.SparkException: Encountered error while reading file file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2010-summary.csv. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,null].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 22 more\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 28 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"count\"\n",
      "\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n",
      "\tat java.base/java.lang.Integer.parseInt(Integer.java:661)\n",
      "\tat java.base/java.lang.Integer.parseInt(Integer.java:777)\n",
      "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n",
      "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n",
      "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n",
      "\t... 31 more\n",
      "24/05/29 11:19:07 WARN TaskSetManager: Lost task 0.0 in stage 44.0 (TID 76) (10.0.2.15 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2010-summary.csv. Details:\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,null].\n",
      "Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n",
      "\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 22 more\n",
      "Caused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n",
      "\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n",
      "\t... 28 more\n",
      "Caused by: java.lang.NumberFormatException: For input string: \"count\"\n",
      "\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n",
      "\tat java.base/java.lang.Integer.parseInt(Integer.java:661)\n",
      "\tat java.base/java.lang.Integer.parseInt(Integer.java:777)\n",
      "\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n",
      "\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n",
      "\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n",
      "\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n",
      "\t... 31 more\n",
      "\n",
      "24/05/29 11:19:07 ERROR TaskSetManager: Task 0 in stage 44.0 failed 1 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o355.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 76) (10.0.2.15 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2010-summary.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 28 more\nCaused by: java.lang.NumberFormatException: For input string: \"count\"\n\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:661)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:777)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 31 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Encountered error while reading file file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2010-summary.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 28 more\nCaused by: java.lang.NumberFormatException: For input string: \"count\"\n\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:661)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:777)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 31 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[77], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:972\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    964\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    965\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    966\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    969\u001b[0m         },\n\u001b[1;32m    970\u001b[0m     )\n\u001b[0;32m--> 972\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, int_truncate, vertical))\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o355.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 44.0 failed 1 times, most recent failure: Lost task 0.0 in stage 44.0 (TID 76) (10.0.2.15 executor driver): org.apache.spark.SparkException: Encountered error while reading file file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2010-summary.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 28 more\nCaused by: java.lang.NumberFormatException: For input string: \"count\"\n\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:661)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:777)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 31 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3326)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3549)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\nCaused by: org.apache.spark.SparkException: Encountered error while reading file file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2010-summary.csv. Details:\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.cannotReadFilesError(QueryExecutionErrors.scala:863)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:293)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:890)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:890)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:364)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:328)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: [MALFORMED_RECORD_IN_PARSING.WITHOUT_SUGGESTION] Malformed records are detected in record parsing: [DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,null].\nParse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'. \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1610)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:79)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:456)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 22 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"count\"\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:365)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:307)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:452)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:60)\n\t... 28 more\nCaused by: java.lang.NumberFormatException: For input string: \"count\"\n\tat java.base/java.lang.NumberFormatException.forInputString(NumberFormatException.java:67)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:661)\n\tat java.base/java.lang.Integer.parseInt(Integer.java:777)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:310)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:310)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:291)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:189)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.org$apache$spark$sql$catalyst$csv$UnivocityParser$$convert(UnivocityParser.scala:346)\n\t... 31 more\n"
     ]
    }
   ],
   "source": [
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f172d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format('csv')\\\n",
    ".schema(myschema)\\\n",
    ".option('mode','dropmalformed')\\\n",
    ".load('csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "edc1aefc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------------+\n",
      "|  DestinationCountry|   OriginCountry|FlightCounts|\n",
      "+--------------------+----------------+------------+\n",
      "|       United States|         Romania|           1|\n",
      "|       United States|         Ireland|         264|\n",
      "|       United States|           India|          69|\n",
      "|               Egypt|   United States|          24|\n",
      "|   Equatorial Guinea|   United States|           1|\n",
      "|       United States|       Singapore|          25|\n",
      "|       United States|         Grenada|          54|\n",
      "|          Costa Rica|   United States|         477|\n",
      "|             Senegal|   United States|          29|\n",
      "|       United States|Marshall Islands|          44|\n",
      "|              Guyana|   United States|          17|\n",
      "|       United States|    Sint Maarten|          53|\n",
      "|               Malta|   United States|           1|\n",
      "|             Bolivia|   United States|          46|\n",
      "|            Anguilla|   United States|          21|\n",
      "|Turks and Caicos ...|   United States|         136|\n",
      "|       United States|     Afghanistan|           2|\n",
      "|Saint Vincent and...|   United States|           1|\n",
      "|               Italy|   United States|         390|\n",
      "|       United States|          Russia|         156|\n",
      "+--------------------+----------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d5be31ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select('OriginCountry','DestinationCountry','FlightCounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "623b07ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- OriginCountry: string (nullable = true)\n",
      " |-- DestinationCountry: string (nullable = true)\n",
      " |-- FlightCounts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5c3f64a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+------------+\n",
      "|   OriginCountry|  DestinationCountry|FlightCounts|\n",
      "+----------------+--------------------+------------+\n",
      "|         Romania|       United States|           1|\n",
      "|         Ireland|       United States|         264|\n",
      "|           India|       United States|          69|\n",
      "|   United States|               Egypt|          24|\n",
      "|   United States|   Equatorial Guinea|           1|\n",
      "|       Singapore|       United States|          25|\n",
      "|         Grenada|       United States|          54|\n",
      "|   United States|          Costa Rica|         477|\n",
      "|   United States|             Senegal|          29|\n",
      "|Marshall Islands|       United States|          44|\n",
      "|   United States|              Guyana|          17|\n",
      "|    Sint Maarten|       United States|          53|\n",
      "|   United States|               Malta|           1|\n",
      "|   United States|             Bolivia|          46|\n",
      "|   United States|            Anguilla|          21|\n",
      "|   United States|Turks and Caicos ...|         136|\n",
      "|     Afghanistan|       United States|           2|\n",
      "|   United States|Saint Vincent and...|           1|\n",
      "|   United States|               Italy|         390|\n",
      "|          Russia|       United States|         156|\n",
      "+----------------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "30355f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfCounts = df.select('FlightCounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b3d4d5a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- FlightCounts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCounts.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "da53bbdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|FlightCounts|\n",
      "+------------+\n",
      "|           1|\n",
      "|         264|\n",
      "|          69|\n",
      "|          24|\n",
      "|           1|\n",
      "|          25|\n",
      "|          54|\n",
      "|         477|\n",
      "|          29|\n",
      "|          44|\n",
      "|          17|\n",
      "|          53|\n",
      "|           1|\n",
      "|          46|\n",
      "|          21|\n",
      "|         136|\n",
      "|           2|\n",
      "|           1|\n",
      "|         390|\n",
      "|         156|\n",
      "+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "75d8d473",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `FlightCounts * 2` cannot be resolved. Did you mean one of the following? [`FlightCounts`, `OriginCountry`, `DestinationCountry`].;\n'Project ['FlightCounts * 2]\n+- Relation [DestinationCountry#851,OriginCountry#852,FlightCounts#853] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[86], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dfDoubleCounts \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlightCounts * 2\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/sql/dataframe.py:3223\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mselect\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39mcols: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mColumnOrName\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   3179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3180\u001b[0m \n\u001b[1;32m   3181\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3221\u001b[0m \u001b[38;5;124;03m    +-----+---+\u001b[39;00m\n\u001b[1;32m   3222\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3223\u001b[0m     jdf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jcols(\u001b[38;5;241m*\u001b[39mcols))\n\u001b[1;32m   3224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(jdf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `FlightCounts * 2` cannot be resolved. Did you mean one of the following? [`FlightCounts`, `OriginCountry`, `DestinationCountry`].;\n'Project ['FlightCounts * 2]\n+- Relation [DestinationCountry#851,OriginCountry#852,FlightCounts#853] csv\n"
     ]
    }
   ],
   "source": [
    "dfDoubleCounts = df.select('FlightCounts * 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7f32c099",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fc99729e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.sql.functions.expr(str: str) -> pyspark.sql.column.Column>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4706a8af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function pyspark.sql.functions.sum(col: 'ColumnOrName') -> pyspark.sql.column.Column>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1810fef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dcc3a29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDoubleCounts = df.select(expr('FlightCounts * 2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "f62f9ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|(FlightCounts * 2)|\n",
      "+------------------+\n",
      "|                 2|\n",
      "|               528|\n",
      "|               138|\n",
      "|                48|\n",
      "|                 2|\n",
      "|                50|\n",
      "|               108|\n",
      "|               954|\n",
      "|                58|\n",
      "|                88|\n",
      "|                34|\n",
      "|               106|\n",
      "|                 2|\n",
      "|                92|\n",
      "|                42|\n",
      "|               272|\n",
      "|                 4|\n",
      "|                 2|\n",
      "|               780|\n",
      "|               312|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDoubleCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "73329a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDoubleCounts = df.select(expr('FlightCounts * 2').alias('DblCounts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "0131624b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|DblCounts|\n",
      "+---------+\n",
      "|        2|\n",
      "|      528|\n",
      "|      138|\n",
      "|       48|\n",
      "|        2|\n",
      "|       50|\n",
      "|      108|\n",
      "|      954|\n",
      "|       58|\n",
      "|       88|\n",
      "|       34|\n",
      "|      106|\n",
      "|        2|\n",
      "|       92|\n",
      "|       42|\n",
      "|      272|\n",
      "|        4|\n",
      "|        2|\n",
      "|      780|\n",
      "|      312|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDoubleCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "92477b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfDoubleCounts = df.select(expr('FlightCounts * 2 as dblcnt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b16779c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|dblcnt|\n",
      "+------+\n",
      "|     2|\n",
      "|   528|\n",
      "|   138|\n",
      "|    48|\n",
      "|     2|\n",
      "|    50|\n",
      "|   108|\n",
      "|   954|\n",
      "|    58|\n",
      "|    88|\n",
      "|    34|\n",
      "|   106|\n",
      "|     2|\n",
      "|    92|\n",
      "|    42|\n",
      "|   272|\n",
      "|     4|\n",
      "|     2|\n",
      "|   780|\n",
      "|   312|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDoubleCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "4347af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|(FlightCounts * 2)|\n",
      "+------------------+\n",
      "|                 2|\n",
      "|               528|\n",
      "|               138|\n",
      "|                48|\n",
      "|                 2|\n",
      "|                50|\n",
      "|               108|\n",
      "|               954|\n",
      "|                58|\n",
      "|                88|\n",
      "|                34|\n",
      "|               106|\n",
      "|                 2|\n",
      "|                92|\n",
      "|                42|\n",
      "|               272|\n",
      "|                 4|\n",
      "|                 2|\n",
      "|               780|\n",
      "|               312|\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDoubleCounts = df.select(df['FlightCounts']*2)\n",
    "dfDoubleCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e39627e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|dblcnt|\n",
      "+------+\n",
      "|     2|\n",
      "|   528|\n",
      "|   138|\n",
      "|    48|\n",
      "|     2|\n",
      "|    50|\n",
      "|   108|\n",
      "|   954|\n",
      "|    58|\n",
      "|    88|\n",
      "|    34|\n",
      "|   106|\n",
      "|     2|\n",
      "|    92|\n",
      "|    42|\n",
      "|   272|\n",
      "|     4|\n",
      "|     2|\n",
      "|   780|\n",
      "|   312|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDoubleCounts = df.select((df['FlightCounts']*2).alias('dblcnt'))\n",
    "dfDoubleCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "444a7b0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'FlightCounts'>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FlightCounts']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "c912c2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'FlightCounts'>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.FlightCounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "557b074e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'FlightCounts'>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col('FlightCounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "9d7a7483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|dblcnt|\n",
      "+------+\n",
      "|     2|\n",
      "|   528|\n",
      "|   138|\n",
      "|    48|\n",
      "|     2|\n",
      "|    50|\n",
      "|   108|\n",
      "|   954|\n",
      "|    58|\n",
      "|    88|\n",
      "|    34|\n",
      "|   106|\n",
      "|     2|\n",
      "|    92|\n",
      "|    42|\n",
      "|   272|\n",
      "|     4|\n",
      "|     2|\n",
      "|   780|\n",
      "|   312|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfDoubleCounts = df.select((col('FlightCounts')*2).alias('dblcnt'))\n",
    "dfDoubleCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "359a4f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|   sqcnt|\n",
      "+--------+\n",
      "|     1.0|\n",
      "| 69696.0|\n",
      "|  4761.0|\n",
      "|   576.0|\n",
      "|     1.0|\n",
      "|   625.0|\n",
      "|  2916.0|\n",
      "|227529.0|\n",
      "|   841.0|\n",
      "|  1936.0|\n",
      "|   289.0|\n",
      "|  2809.0|\n",
      "|     1.0|\n",
      "|  2116.0|\n",
      "|   441.0|\n",
      "| 18496.0|\n",
      "|     4.0|\n",
      "|     1.0|\n",
      "|152100.0|\n",
      "| 24336.0|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsquareCounts = df.select((col('FlightCounts')**2).alias('sqcnt'))\n",
    "dfsquareCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "98b56687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|sqcnt|\n",
      "+-----+\n",
      "|    3|\n",
      "|  266|\n",
      "|   71|\n",
      "|   26|\n",
      "|    3|\n",
      "|   27|\n",
      "|   52|\n",
      "|  479|\n",
      "|   31|\n",
      "|   46|\n",
      "|   19|\n",
      "|   55|\n",
      "|    3|\n",
      "|   44|\n",
      "|   23|\n",
      "|  138|\n",
      "|    0|\n",
      "|    3|\n",
      "|  388|\n",
      "|  158|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsquareCounts = df.select(expr('FlightCounts ^ 2 as sqcnt'))\n",
    "dfsquareCounts.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "411bce10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DestinationCountry: string (nullable = true)\n",
      " |-- OriginCountry: string (nullable = true)\n",
      " |-- FlightCounts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "21f24cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manyflights = df.select('OriginCountry','FlightCounts').where('FlightCounts > 100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "674cfba0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[OriginCountry: string, FlightCounts: int]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_manyflights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7896330b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DestinationCountry: string, OriginCountry: string, FlightCounts: int]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "37137ec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|OriginCountry|FlightCounts|\n",
      "+-------------+------------+\n",
      "|      Ireland|         264|\n",
      "|United States|         477|\n",
      "|United States|         136|\n",
      "|United States|         390|\n",
      "|       Russia|         156|\n",
      "|  Netherlands|         570|\n",
      "|United States|         118|\n",
      "|United States|         391|\n",
      "|United States|         903|\n",
      "|United States|         519|\n",
      "|United States|         315|\n",
      "|United States|         252|\n",
      "|United States|         187|\n",
      "|      Ecuador|         345|\n",
      "|United States|        6200|\n",
      "|United States|         272|\n",
      "|     Portugal|         104|\n",
      "|   Costa Rica|         501|\n",
      "|    Guatemala|         333|\n",
      "|United States|         785|\n",
      "+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_manyflights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "21ef5b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manyflights = df.select('OriginCountry','FlightCounts').where(col('FlightCounts') > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fb784bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+\n",
      "|OriginCountry|FlightCounts|\n",
      "+-------------+------------+\n",
      "|      Ireland|         264|\n",
      "|United States|         477|\n",
      "|United States|         136|\n",
      "|United States|         390|\n",
      "|       Russia|         156|\n",
      "|  Netherlands|         570|\n",
      "|United States|         118|\n",
      "|United States|         391|\n",
      "|United States|         903|\n",
      "|United States|         519|\n",
      "|United States|         315|\n",
      "|United States|         252|\n",
      "|United States|         187|\n",
      "|      Ecuador|         345|\n",
      "|United States|        6200|\n",
      "|United States|         272|\n",
      "|     Portugal|         104|\n",
      "|   Costa Rica|         501|\n",
      "|    Guatemala|         333|\n",
      "|United States|         785|\n",
      "+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_manyflights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "942da60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_manyflights = df.filter(col('FlightCounts') > 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5dea2351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------------+\n",
      "|  DestinationCountry|OriginCountry|FlightCounts|\n",
      "+--------------------+-------------+------------+\n",
      "|       United States|      Ireland|         264|\n",
      "|          Costa Rica|United States|         477|\n",
      "|Turks and Caicos ...|United States|         136|\n",
      "|               Italy|United States|         390|\n",
      "|       United States|       Russia|         156|\n",
      "|       United States|  Netherlands|         570|\n",
      "|             Iceland|United States|         118|\n",
      "|            Honduras|United States|         391|\n",
      "|         The Bahamas|United States|         903|\n",
      "|         El Salvador|United States|         519|\n",
      "|         Switzerland|United States|         315|\n",
      "|           Hong Kong|United States|         252|\n",
      "| Trinidad and Tobago|United States|         187|\n",
      "|       United States|      Ecuador|         345|\n",
      "|              Mexico|United States|        6200|\n",
      "|             Ecuador|United States|         272|\n",
      "|       United States|     Portugal|         104|\n",
      "|       United States|   Costa Rica|         501|\n",
      "|       United States|    Guatemala|         333|\n",
      "|            Colombia|United States|         785|\n",
      "+--------------------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_manyflights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "4abb9d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.select('OriginCountry','DestinationCountry',\n",
    "                  'FlightCounts',(col('FlightCounts')*2).alias('Dblcount'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a1d581ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+------------+--------+\n",
      "|   OriginCountry|  DestinationCountry|FlightCounts|Dblcount|\n",
      "+----------------+--------------------+------------+--------+\n",
      "|         Romania|       United States|           1|       2|\n",
      "|         Ireland|       United States|         264|     528|\n",
      "|           India|       United States|          69|     138|\n",
      "|   United States|               Egypt|          24|      48|\n",
      "|   United States|   Equatorial Guinea|           1|       2|\n",
      "|       Singapore|       United States|          25|      50|\n",
      "|         Grenada|       United States|          54|     108|\n",
      "|   United States|          Costa Rica|         477|     954|\n",
      "|   United States|             Senegal|          29|      58|\n",
      "|Marshall Islands|       United States|          44|      88|\n",
      "|   United States|              Guyana|          17|      34|\n",
      "|    Sint Maarten|       United States|          53|     106|\n",
      "|   United States|               Malta|           1|       2|\n",
      "|   United States|             Bolivia|          46|      92|\n",
      "|   United States|            Anguilla|          21|      42|\n",
      "|   United States|Turks and Caicos ...|         136|     272|\n",
      "|     Afghanistan|       United States|           2|       4|\n",
      "|   United States|Saint Vincent and...|           1|       2|\n",
      "|   United States|               Italy|         390|     780|\n",
      "|          Russia|       United States|         156|     312|\n",
      "+----------------+--------------------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "aff2b931",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.withColumn('Dblcount',col('FlightCounts')*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "ac502daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DestinationCountry: string (nullable = true)\n",
      " |-- OriginCountry: string (nullable = true)\n",
      " |-- FlightCounts: integer (nullable = true)\n",
      " |-- Dblcount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "b64f9faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+------------+--------+\n",
      "|  DestinationCountry|   OriginCountry|FlightCounts|Dblcount|\n",
      "+--------------------+----------------+------------+--------+\n",
      "|       United States|         Romania|           1|       2|\n",
      "|       United States|         Ireland|         264|     528|\n",
      "|       United States|           India|          69|     138|\n",
      "|               Egypt|   United States|          24|      48|\n",
      "|   Equatorial Guinea|   United States|           1|       2|\n",
      "|       United States|       Singapore|          25|      50|\n",
      "|       United States|         Grenada|          54|     108|\n",
      "|          Costa Rica|   United States|         477|     954|\n",
      "|             Senegal|   United States|          29|      58|\n",
      "|       United States|Marshall Islands|          44|      88|\n",
      "|              Guyana|   United States|          17|      34|\n",
      "|       United States|    Sint Maarten|          53|     106|\n",
      "|               Malta|   United States|           1|       2|\n",
      "|             Bolivia|   United States|          46|      92|\n",
      "|            Anguilla|   United States|          21|      42|\n",
      "|Turks and Caicos ...|   United States|         136|     272|\n",
      "|       United States|     Afghanistan|           2|       4|\n",
      "|Saint Vincent and...|   United States|           1|       2|\n",
      "|               Italy|   United States|         390|     780|\n",
      "|       United States|          Russia|         156|     312|\n",
      "+--------------------+----------------+------------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "d8fc21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_2 = df_new.withColumnRenamed('Dblcount','DoublCount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "374a2167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DestinationCountry: string (nullable = true)\n",
      " |-- OriginCountry: string (nullable = true)\n",
      " |-- FlightCounts: integer (nullable = true)\n",
      " |-- Dblcount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c9f8af77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DestinationCountry: string (nullable = true)\n",
      " |-- OriginCountry: string (nullable = true)\n",
      " |-- FlightCounts: integer (nullable = true)\n",
      " |-- DoublCount: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_new_2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c45f0f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlightGroups =  df.groupBy('DestinationCountry','OriginCountry').avg('FlightCounts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "a70eb609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-------------+------------------+\n",
      "|DestinationCountry|OriginCountry| avg(FlightCounts)|\n",
      "+------------------+-------------+------------------+\n",
      "|           Croatia|United States|1.6666666666666667|\n",
      "|            Kosovo|United States|               1.0|\n",
      "|           Romania|United States|             10.75|\n",
      "|           Ireland|United States|             265.5|\n",
      "|     United States|        Egypt|              14.5|\n",
      "|             India|United States|63.833333333333336|\n",
      "|             Niger|United States|               1.5|\n",
      "|         Singapore|United States|              20.4|\n",
      "|           Grenada|United States|              53.5|\n",
      "|     United States|   Costa Rica| 549.8333333333334|\n",
      "|     United States|      Senegal|35.666666666666664|\n",
      "|  Marshall Islands|United States|59.333333333333336|\n",
      "|      Sint Maarten|United States|238.33333333333334|\n",
      "|     United States|        Malta|              2.25|\n",
      "|     United States|       Guyana|49.666666666666664|\n",
      "|     United States|   Montenegro|               1.0|\n",
      "|     United States|     Anguilla|              25.0|\n",
      "|     United States|      Bolivia|25.166666666666668|\n",
      "|     United States|      Algeria|               1.0|\n",
      "|          Paraguay|United States| 80.83333333333333|\n",
      "+------------------+-------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFlightGroups.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f5e13964",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlightGroups =  df.groupBy('DestinationCountry','OriginCountry')\\\n",
    ".agg(avg('FlightCounts'),F.max('FlightCounts'),median('FlightCounts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "01e52789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DestinationCountry: string (nullable = true)\n",
      " |-- OriginCountry: string (nullable = true)\n",
      " |-- avg(FlightCounts): double (nullable = true)\n",
      " |-- max(FlightCounts): integer (nullable = true)\n",
      " |-- median(FlightCounts): double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFlightGroups.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "a30f9d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfFlightGroups =  df.groupBy('DestinationCountry','OriginCountry')\\\n",
    ".agg(avg('FlightCounts').alias('AvgCount'),\n",
    "     F.max('FlightCounts').alias('MaxCount'),\n",
    "     median('FlightCounts').alias('MedianCounts'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "1fb872a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DestinationCountry: string (nullable = true)\n",
      " |-- OriginCountry: string (nullable = true)\n",
      " |-- AvgCount: double (nullable = true)\n",
      " |-- MaxCount: integer (nullable = true)\n",
      " |-- MedianCounts: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFlightGroups.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "bebf8ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-------------+------------------+--------+------------+\n",
      "|DestinationCountry               |OriginCountry|AvgCount          |MaxCount|MedianCounts|\n",
      "+---------------------------------+-------------+------------------+--------+------------+\n",
      "|Afghanistan                      |United States|8.0               |11      |8.0         |\n",
      "|Algeria                          |United States|5.0               |9       |4.0         |\n",
      "|Angola                           |United States|13.166666666666666|15      |13.0        |\n",
      "|Anguilla                         |United States|26.333333333333332|41      |21.5        |\n",
      "|Antigua and Barbuda              |United States|129.66666666666666|146     |124.5       |\n",
      "|Argentina                        |United States|187.66666666666666|208     |183.5       |\n",
      "|Aruba                            |United States|350.6666666666667 |359     |350.0       |\n",
      "|Australia                        |United States|294.0             |329     |291.5       |\n",
      "|Austria                          |United States|41.333333333333336|62      |35.5        |\n",
      "|Azerbaijan                       |United States|8.0               |21      |5.0         |\n",
      "|Bahrain                          |United States|40.833333333333336|65      |40.5        |\n",
      "|Barbados                         |United States|122.16666666666667|154     |125.0       |\n",
      "|Belarus                          |United States|1.0               |1       |1.0         |\n",
      "|Belgium                          |United States|319.8333333333333 |408     |308.5       |\n",
      "|Belize                           |United States|135.66666666666666|188     |129.0       |\n",
      "|Bermuda                          |United States|182.5             |191     |184.0       |\n",
      "|Bolivia                          |United States|39.666666666666664|61      |34.0        |\n",
      "|Bonaire, Sint Eustatius, and Saba|United States|47.5              |62      |51.5        |\n",
      "|Brazil                           |United States|939.1666666666666 |995     |948.0       |\n",
      "|British Virgin Islands           |United States|86.83333333333333 |108     |92.0        |\n",
      "+---------------------------------+-------------+------------------+--------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfFlightGroups.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "ed00dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNull = spark.read.csv('NullData.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ef7d04e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+\n",
      "|  Id| Name|Sales|\n",
      "+----+-----+-----+\n",
      "|emp1| John| NULL|\n",
      "|emp2| NULL| NULL|\n",
      "|emp3| NULL|345.0|\n",
      "|emp4|Cindy|456.0|\n",
      "+----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfNull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "d3ae6e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_null = dfNull.withColumn('DoubleSale',expr('Sales * 2 as dblsale'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "f9f2e806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|  Id| Name|Sales|DoubleSale|\n",
      "+----+-----+-----+----------+\n",
      "|emp1| John| NULL|      NULL|\n",
      "|emp2| NULL| NULL|      NULL|\n",
      "|emp3| NULL|345.0|     690.0|\n",
      "|emp4|Cindy|456.0|     912.0|\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "be2f7fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notnull = df_null.na.drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "df41001e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|  Id| Name|Sales|DoubleSale|\n",
      "+----+-----+-----+----------+\n",
      "|emp4|Cindy|456.0|     912.0|\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "c094a4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notnull = df_null.na.drop(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2b89e0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|  Id| Name|Sales|DoubleSale|\n",
      "+----+-----+-----+----------+\n",
      "|emp1| John| NULL|      NULL|\n",
      "|emp2| NULL| NULL|      NULL|\n",
      "|emp3| NULL|345.0|     690.0|\n",
      "|emp4|Cindy|456.0|     912.0|\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "78ae6979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|  Id| Name|Sales|DoubleSale|\n",
      "+----+-----+-----+----------+\n",
      "|emp1| John| NULL|      NULL|\n",
      "|emp3| NULL|345.0|     690.0|\n",
      "|emp4|Cindy|456.0|     912.0|\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notnull = df_null.na.drop(subset=['Name','Sales','DoubleSale'],how='all')\n",
    "df_notnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f1fed44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|  Id| Name|Sales|DoubleSale|\n",
      "+----+-----+-----+----------+\n",
      "|emp1| John| NULL|      NULL|\n",
      "|emp2| NULL| NULL|      NULL|\n",
      "|emp3| NULL|345.0|     690.0|\n",
      "|emp4|Cindy|456.0|     912.0|\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "ef93e7b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|  Id| Name|Sales|DoubleSale|\n",
      "+----+-----+-----+----------+\n",
      "|emp1| John| NULL|      NULL|\n",
      "|emp3| NULL|345.0|     690.0|\n",
      "|emp4|Cindy|456.0|     912.0|\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notnull = df_null.na.drop(thresh=2)\n",
    "df_notnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "5189a1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|  Id| Name|Sales|DoubleSale|\n",
      "+----+-----+-----+----------+\n",
      "|emp3| NULL|345.0|     690.0|\n",
      "|emp4|Cindy|456.0|     912.0|\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notnull = df_null.na.drop(thresh=3)\n",
    "df_notnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d3c62977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|  Id| Name|Sales|DoubleSale|\n",
      "+----+-----+-----+----------+\n",
      "|emp1| John| NULL|      NULL|\n",
      "|emp2| NULL| NULL|      NULL|\n",
      "|emp3| NULL|345.0|     690.0|\n",
      "|emp4|Cindy|456.0|     912.0|\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_null.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "c0c5a403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|  Id| Name|Sales|DoubleSale|\n",
      "+----+-----+-----+----------+\n",
      "|emp3| NULL|345.0|     690.0|\n",
      "|emp4|Cindy|456.0|     912.0|\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_notnull = df_null.na.drop(subset=['Name','Sales','DoubleSale'],thresh=2)\n",
    "df_notnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "bc6af811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|  Id| Name|Sales|DoubleSale|\n",
      "+----+-----+-----+----------+\n",
      "|emp1| John| 15.0|      15.0|\n",
      "|emp2| NULL| 15.0|      15.0|\n",
      "|emp3| NULL|345.0|     690.0|\n",
      "|emp4|Cindy|456.0|     912.0|\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fillnull = df_null.na.fill(15)\n",
    "df_fillnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cd1c9792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+----------+\n",
      "|  Id|   Name|Sales|DoubleSale|\n",
      "+----+-------+-----+----------+\n",
      "|emp1|   John| NULL|      NULL|\n",
      "|emp2|No Name| NULL|      NULL|\n",
      "|emp3|No Name|345.0|     690.0|\n",
      "|emp4|  Cindy|456.0|     912.0|\n",
      "+----+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fillnull = df_null.na.fill('No Name')\n",
    "df_fillnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "4b93ef01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "225"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "15*15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "9235d7a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|  Id| Name|Sales|DoubleSale|\n",
      "+----+-----+-----+----------+\n",
      "|emp1| John| 15.0|     225.0|\n",
      "|emp2| NULL| 15.0|     225.0|\n",
      "|emp3| NULL|345.0|     690.0|\n",
      "|emp4|Cindy|456.0|     912.0|\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fillnull = df_null.na.fill(15,subset=['Sales']).na.fill(225,subset=['DoubleSale'])\n",
    "df_fillnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "22aaa6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+-----+----------+\n",
      "|  Id|   Name|Sales|DoubleSale|\n",
      "+----+-------+-----+----------+\n",
      "|emp1|   John| 15.0|     225.0|\n",
      "|emp2|No Name| 15.0|     225.0|\n",
      "|emp3|No Name|345.0|     690.0|\n",
      "|emp4|  Cindy|456.0|     912.0|\n",
      "+----+-------+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_fillnull = df_null.na.fill({'Name':'No Name','Sales':15,'DoubleSale':225})\n",
    "df_fillnull.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "7e7b88f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Id: string, Name: string, Sales: double, DoubleSale: double]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_fillnull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "6f40581c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DestinationCountry: string (nullable = true)\n",
      " |-- OriginCountry: string (nullable = true)\n",
      " |-- FlightCounts: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "57197a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('myview')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a2cc2a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ManyFlights = spark.sql('select * from myview where FlightCounts>100')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0595438c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+------------+\n",
      "|  DestinationCountry|OriginCountry|FlightCounts|\n",
      "+--------------------+-------------+------------+\n",
      "|       United States|      Ireland|         264|\n",
      "|          Costa Rica|United States|         477|\n",
      "|Turks and Caicos ...|United States|         136|\n",
      "|               Italy|United States|         390|\n",
      "|       United States|       Russia|         156|\n",
      "|       United States|  Netherlands|         570|\n",
      "|             Iceland|United States|         118|\n",
      "|            Honduras|United States|         391|\n",
      "|         The Bahamas|United States|         903|\n",
      "|         El Salvador|United States|         519|\n",
      "|         Switzerland|United States|         315|\n",
      "|           Hong Kong|United States|         252|\n",
      "| Trinidad and Tobago|United States|         187|\n",
      "|       United States|      Ecuador|         345|\n",
      "|              Mexico|United States|        6200|\n",
      "|             Ecuador|United States|         272|\n",
      "|       United States|     Portugal|         104|\n",
      "|       United States|   Costa Rica|         501|\n",
      "|       United States|    Guatemala|         333|\n",
      "|            Colombia|United States|         785|\n",
      "+--------------------+-------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/29 12:42:20 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2010-summary.csv\n"
     ]
    }
   ],
   "source": [
    "df_ManyFlights.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "590c26b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/29 13:00:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2011-summary.csv\n",
      "24/05/29 13:00:53 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 2, schema size: 3\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2009-summary.csv\n",
      "24/05/29 13:00:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2012-summary.csv\n",
      "24/05/29 13:00:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2010-summary.csv\n",
      "24/05/29 13:00:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2015-summary.csv\n",
      "24/05/29 13:00:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2014-summary.csv\n",
      "24/05/29 13:00:53 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2013-summary.csv\n"
     ]
    }
   ],
   "source": [
    "df_ManyFlights.write.format('parquet').\\\n",
    "mode('overwrite').\\\n",
    "option('path','manyflights').\\\n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5294cf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/29 13:02:15 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 2, schema size: 3\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2009-summary.csv\n",
      "24/05/29 13:02:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2012-summary.csv\n",
      "24/05/29 13:02:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2011-summary.csv\n",
      "24/05/29 13:02:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2010-summary.csv\n",
      "24/05/29 13:02:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2013-summary.csv\n",
      "24/05/29 13:02:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2015-summary.csv\n",
      "24/05/29 13:02:15 WARN CSVHeaderChecker: CSV header does not conform to the schema.\n",
      " Header: DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count\n",
      " Schema: DestinationCountry, OriginCountry, FlightCounts\n",
      "Expected: DestinationCountry but found: DEST_COUNTRY_NAME\n",
      "CSV file: file:///home/hatem/PySpark/Intake%2044/L2_RDD_DataFrames/csv/2014-summary.csv\n"
     ]
    }
   ],
   "source": [
    "df_ManyFlights.write.format('jdbc').\\\n",
    "option('url','jdbc:postgresql://localhost:5432/database_example').\\\n",
    "option('dbtable','Dina').\\\n",
    "option('user','postgres').\\\n",
    "option('password','DM$1234').\\\n",
    "save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "7727205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_post = spark.read.format('jdbc').\\\n",
    "option('url','jdbc:postgresql://localhost:5432/database_example').\\\n",
    "option('dbtable','departuredelays').\\\n",
    "option('user','postgres').\\\n",
    "option('password','DM$1234').\\\n",
    "load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "bcea3b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: integer (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_post.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "7dd5c363",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 88:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|1010630|  -10|     928|   RSW|        EWR|\n",
      "|1021029|   87|     974|   RSW|        ORD|\n",
      "|1021346|    0|     928|   RSW|        EWR|\n",
      "|1021044|   18|     928|   RSW|        EWR|\n",
      "|1021730|   29|     748|   RSW|        IAH|\n",
      "|1020535|  605|     974|   RSW|        ORD|\n",
      "|1021820|   71|     974|   RSW|        ORD|\n",
      "|1021743|    0|     928|   RSW|        EWR|\n",
      "|1022017|    0|     928|   RSW|        EWR|\n",
      "|1020600|   -2|     748|   RSW|        IAH|\n",
      "|1021214|   29|     891|   RSW|        CLE|\n",
      "|1020630|   -5|     928|   RSW|        EWR|\n",
      "|1031029|   13|     974|   RSW|        ORD|\n",
      "|1031346|  279|     928|   RSW|        EWR|\n",
      "|1031740|   29|     748|   RSW|        IAH|\n",
      "|1030535|    0|     974|   RSW|        ORD|\n",
      "|1031808|   -3|     974|   RSW|        ORD|\n",
      "|1031516|   -2|    1396|   RSW|        DEN|\n",
      "|1032017|   14|     928|   RSW|        EWR|\n",
      "|1031214|   17|     891|   RSW|        CLE|\n",
      "+-------+-----+--------+------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_post.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a77ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
