{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31e52f91",
   "metadata": {},
   "source": [
    "### Writing to files\n",
    "- Structured Streaming supports writing streaming query output to files in the same formats as reads. However, it only supports append mode, because while it is easy to write new files in the output directory (i.e., append data to a directory), it is hard to modify existing data files (as would be expected with update and complete modes). It also supports partitioning.\n",
    "- For <b>Memort Sink</b>, it supports <b>Append, Complete</b> modes.\n",
    "- For <b>Console Sink</b>, it supports <b>Append, Complete, and Update</b> modes.\n",
    "- However, Console and Memory Sinks are usually used only for debugging. Such as <b>.show()</b> in static tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10fafb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Writing to Memory\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a5be397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/07 14:43:34 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "df = spark.readStream.format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 12345) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3dfc2277",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"memory\")  \\\n",
    "    .queryName('myTable')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "703b8646",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/07 14:43:59 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-32b394ab-ddbc-4877-b245-1e274c507d44. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/07 14:43:59 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query= writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86faa562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for data to arrive',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|hi hello hi hello|\n",
      "|      hi hello hi|\n",
      "|               hi|\n",
      "|             ds42|\n",
      "|                 |\n",
      "|      hi hello hi|\n",
      "|             ds44|\n",
      "|      hi hello hi|\n",
      "|             ds44|\n",
      "|               hi|\n",
      "|      hi hello hi|\n",
      "|               hi|\n",
      "|             ds44|\n",
      "|             ds43|\n",
      "|            hello|\n",
      "|      hi hello hi|\n",
      "|            hello|\n",
      "|             ds43|\n",
      "|             ds42|\n",
      "|hi hello hi hello|\n",
      "+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m display(query\u001b[38;5;241m.\u001b[39mstatus)\n\u001b[1;32m      7\u001b[0m display(spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSELECT * FROM myTable\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mshow())\n\u001b[0;32m----> 8\u001b[0m sleep(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import display, clear_output\n",
    "from time import sleep\n",
    "\n",
    "while True:\n",
    "    clear_output(wait=True)\n",
    "    display(query.status)\n",
    "    display(spark.sql('SELECT * FROM myTable').show())\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa4a579a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|hi hello hi hello|\n",
      "|      hi hello hi|\n",
      "|               hi|\n",
      "|             ds42|\n",
      "|                 |\n",
      "|      hi hello hi|\n",
      "|             ds44|\n",
      "|      hi hello hi|\n",
      "|             ds44|\n",
      "|               hi|\n",
      "|      hi hello hi|\n",
      "|               hi|\n",
      "|             ds44|\n",
      "|             ds43|\n",
      "|            hello|\n",
      "|      hi hello hi|\n",
      "|            hello|\n",
      "|             ds43|\n",
      "|             ds42|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM myTable').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ff9058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d052302",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd15f62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
