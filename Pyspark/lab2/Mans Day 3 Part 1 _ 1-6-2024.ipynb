{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c6ceb45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58691037",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x74968030b150>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08566dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfStat = spark.read.format('csv').load('D1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee7b7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------+------+-----------+\n",
      "|    _c0|  _c1|     _c2|   _c3|        _c4|\n",
      "+-------+-----+--------+------+-----------+\n",
      "|   date|delay|distance|origin|destination|\n",
      "|1011245|    6|     602|   ABE|        ATL|\n",
      "|1020600|   -8|     369|   ABE|        DTW|\n",
      "|1021245|   -2|     602|   ABE|        ATL|\n",
      "|1020605|   -4|     602|   ABE|        ATL|\n",
      "|1031245|   -4|     602|   ABE|        ATL|\n",
      "|1030605|    0|     602|   ABE|        ATL|\n",
      "|1041243|   10|     602|   ABE|        ATL|\n",
      "|1040605|   28|     602|   ABE|        ATL|\n",
      "|1051245|   88|     602|   ABE|        ATL|\n",
      "|1050605|    9|     602|   ABE|        ATL|\n",
      "|1061215|   -6|     602|   ABE|        ATL|\n",
      "|1061725|   69|     602|   ABE|        ATL|\n",
      "|1061230|    0|     369|   ABE|        DTW|\n",
      "|1060625|   -3|     602|   ABE|        ATL|\n",
      "|1070600|    0|     369|   ABE|        DTW|\n",
      "|1071725|    0|     602|   ABE|        ATL|\n",
      "|1071230|    0|     369|   ABE|        DTW|\n",
      "|1070625|    0|     602|   ABE|        ATL|\n",
      "|1071219|    0|     569|   ABE|        ORD|\n",
      "+-------+-----+--------+------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfStat.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48d8772a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfStat.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "462d19c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 11:14:47 WARN TextSocketSourceProvider: The socket source should not be used for production applications! It does not support recovery.\n"
     ]
    }
   ],
   "source": [
    "df = spark.readStream.format('socket')\\\n",
    ".option('host','localhost')\\\n",
    ".option('port',12345)\\\n",
    ".load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c38d063e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74d05c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9546fe62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "431ef408",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsplit = df.withColumn('Splitteddata',split('value',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "666f6826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- Splitteddata: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsplit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "711e8846",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsize = dfsplit.withColumn('Size',size('Splitteddata'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85befb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- Splitteddata: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      " |-- Size: integer (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsize.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3ef5fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = dfsize.writeStream.format('console')\\\n",
    ".outputMode('append')\\\n",
    ".option('truncate','false')\\\n",
    ".trigger(processingTime='2 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6b58951",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.readwriter.DataStreamWriter at 0x74965f62a850>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2319b24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 11:33:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-9dc42b38-fd60-48c7-8e0a-9b0b04c045a4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/01 11:33:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+------------+----+\n",
      "|value|Splitteddata|Size|\n",
      "+-----+------------+----+\n",
      "+-----+------------+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|value                                                                                                                                                |Splitteddata                                                                                                                                                                  |Size|\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|Hi how are you?                                                                                                                                      |[Hi, how, are, you?]                                                                                                                                                          |4   |\n",
      "|The Spark SQL engine will take care of running it incrementally and continuously and updating the final result as streaming data continues to arrive.|[The, Spark, SQL, engine, will, take, care, of, running, it, incrementally, and, continuously, and, updating, the, final, result, as, streaming, data, continues, to, arrive.]|24  |\n",
      "|Hello guys                                                                                                                                           |[Hello, guys]                                                                                                                                                                 |2   |\n",
      "|You can use the Dataset/DataFrame API in Scala, Java, Python or R to express streaming aggregations, event-time windows, stream-to-batch joins, etc. |[You, can, use, the, Dataset/DataFrame, API, in, Scala,, Java,, Python, or, R, to, express, streaming, aggregations,, event-time, windows,, stream-to-batch, joins,, etc.]    |21  |\n",
      "|Structured Streaming is a scalable and fault-tolerant stream processing engine built on the Spark SQL engine.                                        |[Structured, Streaming, is, a, scalable, and, fault-tolerant, stream, processing, engine, built, on, the, Spark, SQL, engine.]                                                |16  |\n",
      "|The computation is executed on the same optimized Spark SQL engine.                                                                                  |[The, computation, is, executed, on, the, same, optimized, Spark, SQL, engine.]                                                                                               |11  |\n",
      "|You can express your streaming computation the same way you would express a batch computation on static data.                                        |[You, can, express, your, streaming, computation, the, same, way, you, would, express, a, batch, computation, on, static, data.]                                              |18  |\n",
      "|Finally, the system ensures end-to-end exactly-once fault-tolerance guarantees through checkpointing and Write-Ahead Logs.                           |[Finally,, the, system, ensures, end-to-end, exactly-once, fault-tolerance, guarantees, through, checkpointing, and, Write-Ahead, Logs., ]                                    |14  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|value                                                                                                                                                                                                                                                                                                 |Splitteddata                                                                                                                                                                                                                                                                                                                                             |Size|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|Unsupported Operations                                                                                                                                                                                                                                                                                |[Unsupported, Operations]                                                                                                                                                                                                                                                                                                                                |2   |\n",
      "|    Distinct operations on streaming Datasets are not supported.                                                                                                                                                                                                                                      |[, , , , Distinct, operations, on, streaming, Datasets, are, not, supported.]                                                                                                                                                                                                                                                                            |12  |\n",
      "|    In addition, there are some Dataset methods that will not work on streaming Datasets. They are actions that will immediately run queries and return results, which does not make sense on a streaming Dataset. Rather, those functionalities can be done by explicitly starting a streaming query.|[, , , , In, addition,, there, are, some, Dataset, methods, that, will, not, work, on, streaming, Datasets., They, are, actions, that, will, immediately, run, queries, and, return, results,, which, does, not, make, sense, on, a, streaming, Dataset., Rather,, those, functionalities, can, be, done, by, explicitly, starting, a, streaming, query.]|50  |\n",
      "|                                                                                                                                                                                                                                                                                                      |[]                                                                                                                                                                                                                                                                                                                                                       |1   |\n",
      "|    Deduplication operation is not supported after aggregation on a streaming Datasets.                                                                                                                                                                                                               |[, , , , Deduplication, operation, is, not, supported, after, aggregation, on, a, streaming, Datasets.]                                                                                                                                                                                                                                                  |15  |\n",
      "|        count() - Cannot return a single count from a streaming Dataset. Instead, use ds.groupBy().count() which returns a streaming Dataset containing a running count.                                                                                                                              |[, , , , , , , , count(), -, Cannot, return, a, single, count, from, a, streaming, Dataset., Instead,, use, ds.groupBy().count(), which, returns, a, streaming, Dataset, containing, a, running, count.]                                                                                                                                                 |31  |\n",
      "|    Multiple streaming aggregations (i.e. a chain of aggregations on a streaming DF) are not yet supported on streaming Datasets.                                                                                                                                                                     |[, , , , Multiple, streaming, aggregations, (i.e., a, chain, of, aggregations, on, a, streaming, DF), are, not, yet, supported, on, streaming, Datasets.]                                                                                                                                                                                                |23  |\n",
      "|    Sorting operations are supported on streaming Datasets only after an aggregation and in Complete Output Mode.                                                                                                                                                                                     |[, , , , Sorting, operations, are, supported, on, streaming, Datasets, only, after, an, aggregation, and, in, Complete, Output, Mode.]                                                                                                                                                                                                                   |20  |\n",
      "|        foreach() - Instead use ds.writeStream.foreach(...).                                                                                                                                                                                                                                          |[, , , , , , , , foreach(), -, Instead, use, ds.writeStream.foreach(...).]                                                                                                                                                                                                                                                                               |13  |\n",
      "|    Limit and take the first N rows are not supported on streaming Datasets.                                                                                                                                                                                                                          |[, , , , Limit, and, take, the, first, N, rows, are, not, supported, on, streaming, Datasets.]                                                                                                                                                                                                                                                           |17  |\n",
      "|    Few types of outer joins on streaming Datasets are not supported.                                                                                                                                                                                                                                 |[, , , , Few, types, of, outer, joins, on, streaming, Datasets, are, not, supported.]                                                                                                                                                                                                                                                                    |15  |\n",
      "|        show() - Instead use the console sink.                                                                                                                                                                                                                                                        |[, , , , , , , , show(), -, Instead, use, the, console, sink.]                                                                                                                                                                                                                                                                                           |15  |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+-----+------------+----+\n",
      "|value|Splitteddata|Size|\n",
      "+-----+------------+----+\n",
      "|     |[]          |1   |\n",
      "+-----+------------+----+\n",
      "\n",
      "-------------------------------------------\n",
      "Batch: 4\n",
      "-------------------------------------------\n",
      "+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|value                                                                                                  |Splitteddata                                                                                                               |Size|\n",
      "+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+----+\n",
      "|#### You can also register a streaming DataFrame as a temporary view and then apply SQL commands on it.|[####, You, can, also, register, a, streaming, DataFrame, as, a, temporary, view, and, then, apply, SQL, commands, on, it.]|19  |\n",
      "+-------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b04d69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f11fd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      " |-- Splitteddata: array (nullable = true)\n",
      " |    |-- element: string (containsNull = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfsplit.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c07ad59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfwords = dfsplit.select(explode('Splitteddata').alias('Words'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24de63a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Words: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfwords.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bb76b40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfcount = dfwords.groupBy('Words').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "24640bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Words: string (nullable = false)\n",
      " |-- count: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcount.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f42ff57",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = dfcount.writeStream.format('console')\\\n",
    ".outputMode('complete')\\\n",
    ".option('truncate','false')\\\n",
    ".trigger(processingTime='2 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2eb4b868",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 11:41:23 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-8da7dd64-031b-4779-8687-802593dcfaff. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/01 11:41:23 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|Words|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 11:41:40 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 16548 milliseconds\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|Words|count|\n",
      "+-----+-----+\n",
      "|you  |1    |\n",
      "|how  |1    |\n",
      "|Hi   |1    |\n",
      "|are  |1    |\n",
      "|all  |1    |\n",
      "|?    |1    |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 11:42:58 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 10660 milliseconds\n",
      "24/06/01 11:43:45 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 9964 milliseconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|Words|count|\n",
      "+-----+-----+\n",
      "|you  |1    |\n",
      "|how  |2    |\n",
      "|Hi   |2    |\n",
      "|are  |1    |\n",
      "|all  |1    |\n",
      "|?    |1    |\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 3\n",
      "-------------------------------------------\n",
      "+---------------+-----+\n",
      "|Words          |count|\n",
      "+---------------+-----+\n",
      "|those          |1    |\n",
      "|some           |1    |\n",
      "|Cannot         |1    |\n",
      "|operation      |1    |\n",
      "|joins          |1    |\n",
      "|Instead,       |1    |\n",
      "|sense          |1    |\n",
      "|count()        |1    |\n",
      "|returns        |1    |\n",
      "|aggregations   |2    |\n",
      "|starting       |1    |\n",
      "|not            |7    |\n",
      "|Distinct       |1    |\n",
      "|will           |2    |\n",
      "|functionalities|1    |\n",
      "|by             |1    |\n",
      "|Rather,        |1    |\n",
      "|you            |1    |\n",
      "|outer          |1    |\n",
      "|Deduplication  |1    |\n",
      "+---------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 11:45:09 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 7765 milliseconds\n"
     ]
    }
   ],
   "source": [
    "q = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7114fc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be208edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = dfcount.writeStream.format('console')\\\n",
    ".outputMode('complete')\\\n",
    ".option('truncate','false')\\\n",
    ".option('numrows',100)\\\n",
    ".trigger(processingTime='20 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e9b0f4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 11:47:58 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-be04ee91-f227-456f-9804-99cf91b84f33. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/01 11:47:58 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|Words|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+----------------------------+-----+\n",
      "|Words                       |count|\n",
      "+----------------------------+-----+\n",
      "|those                       |1    |\n",
      "|some                        |1    |\n",
      "|Cannot                      |1    |\n",
      "|operation                   |1    |\n",
      "|joins                       |1    |\n",
      "|Instead,                    |1    |\n",
      "|sense                       |1    |\n",
      "|count()                     |1    |\n",
      "|returns                     |1    |\n",
      "|aggregations                |2    |\n",
      "|starting                    |1    |\n",
      "|not                         |7    |\n",
      "|Distinct                    |1    |\n",
      "|will                        |2    |\n",
      "|functionalities             |1    |\n",
      "|by                          |1    |\n",
      "|Rather,                     |1    |\n",
      "|you                         |1    |\n",
      "|outer                       |1    |\n",
      "|Deduplication               |1    |\n",
      "|In                          |1    |\n",
      "|there                       |1    |\n",
      "|Complete                    |1    |\n",
      "|can                         |2    |\n",
      "|apply                       |1    |\n",
      "|how                         |2    |\n",
      "|Instead                     |2    |\n",
      "|ds.writeStream.foreach(...).|1    |\n",
      "|Output                      |1    |\n",
      "|in                          |1    |\n",
      "|sink.                       |1    |\n",
      "|DF)                         |1    |\n",
      "|count                       |1    |\n",
      "|take                        |1    |\n",
      "|be                          |1    |\n",
      "|supported.                  |2    |\n",
      "|streaming                   |14   |\n",
      "|types                       |1    |\n",
      "|addition,                   |1    |\n",
      "|commands                    |1    |\n",
      "|(i.e.                       |1    |\n",
      "|rows                        |1    |\n",
      "|Few                         |1    |\n",
      "|immediately                 |1    |\n",
      "|ds.groupBy().count()        |1    |\n",
      "|after                       |2    |\n",
      "|query.                      |1    |\n",
      "|Hi                          |2    |\n",
      "|is                          |1    |\n",
      "|on                          |10   |\n",
      "|aggregation                 |2    |\n",
      "|use                         |3    |\n",
      "|view                        |1    |\n",
      "|actions                     |1    |\n",
      "|Sorting                     |1    |\n",
      "|Dataset.                    |2    |\n",
      "|count.                      |1    |\n",
      "|does                        |1    |\n",
      "|-                           |3    |\n",
      "|only                        |1    |\n",
      "|chain                       |1    |\n",
      "|You                         |1    |\n",
      "|Mode.                       |1    |\n",
      "|queries                     |1    |\n",
      "|DataFrame                   |1    |\n",
      "|done                        |1    |\n",
      "|the                         |2    |\n",
      "|N                           |1    |\n",
      "|Dataset                     |2    |\n",
      "|return                      |2    |\n",
      "|from                        |1    |\n",
      "|containing                  |1    |\n",
      "|temporary                   |1    |\n",
      "|Limit                       |1    |\n",
      "|make                        |1    |\n",
      "|work                        |1    |\n",
      "|methods                     |1    |\n",
      "|single                      |1    |\n",
      "|and                         |4    |\n",
      "|it.                         |1    |\n",
      "|operations                  |2    |\n",
      "|are                         |8    |\n",
      "|Datasets                    |3    |\n",
      "|SQL                         |1    |\n",
      "|of                          |2    |\n",
      "|Datasets.                   |4    |\n",
      "|results,                    |1    |\n",
      "|console                     |1    |\n",
      "|yet                         |1    |\n",
      "|an                          |1    |\n",
      "|###                         |1    |\n",
      "|that                        |2    |\n",
      "|running                     |1    |\n",
      "|run                         |1    |\n",
      "|all                         |1    |\n",
      "|a                           |11   |\n",
      "|as                          |1    |\n",
      "|                            |48   |\n",
      "|register                    |1    |\n",
      "|show()                      |1    |\n",
      "+----------------------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+----------------------------+-----+\n",
      "|Words                       |count|\n",
      "+----------------------------+-----+\n",
      "|Cannot                      |1    |\n",
      "|those                       |1    |\n",
      "|some                        |1    |\n",
      "|operation                   |1    |\n",
      "|Instead,                    |1    |\n",
      "|joins                       |1    |\n",
      "|sense                       |1    |\n",
      "|count()                     |1    |\n",
      "|returns                     |1    |\n",
      "|aggregations                |2    |\n",
      "|starting                    |1    |\n",
      "|not                         |7    |\n",
      "|Distinct                    |1    |\n",
      "|will                        |2    |\n",
      "|functionalities             |1    |\n",
      "|by                          |1    |\n",
      "|Rather,                     |1    |\n",
      "|you                         |1    |\n",
      "|outer                       |1    |\n",
      "|Deduplication               |1    |\n",
      "|In                          |1    |\n",
      "|there                       |1    |\n",
      "|Complete                    |1    |\n",
      "|can                         |2    |\n",
      "|apply                       |1    |\n",
      "|how                         |2    |\n",
      "|Instead                     |2    |\n",
      "|ds.writeStream.foreach(...).|1    |\n",
      "|Output                      |1    |\n",
      "|in                          |1    |\n",
      "|sink.                       |1    |\n",
      "|DF)                         |1    |\n",
      "|count                       |1    |\n",
      "|take                        |1    |\n",
      "|be                          |1    |\n",
      "|supported.                  |2    |\n",
      "|streaming                   |14   |\n",
      "|types                       |1    |\n",
      "|addition,                   |1    |\n",
      "|commands                    |1    |\n",
      "|(i.e.                       |1    |\n",
      "|rows                        |1    |\n",
      "|Few                         |1    |\n",
      "|immediately                 |1    |\n",
      "|ds.groupBy().count()        |1    |\n",
      "|after                       |2    |\n",
      "|query.                      |1    |\n",
      "|Hi                          |2    |\n",
      "|is                          |1    |\n",
      "|on                          |10   |\n",
      "|aggregation                 |2    |\n",
      "|use                         |3    |\n",
      "|view                        |1    |\n",
      "|actions                     |1    |\n",
      "|Sorting                     |1    |\n",
      "|Dataset.                    |2    |\n",
      "|count.                      |1    |\n",
      "|does                        |1    |\n",
      "|-                           |3    |\n",
      "|only                        |1    |\n",
      "|chain                       |1    |\n",
      "|You                         |1    |\n",
      "|Mode.                       |1    |\n",
      "|queries                     |1    |\n",
      "|DataFrame                   |1    |\n",
      "|done                        |1    |\n",
      "|the                         |2    |\n",
      "|N                           |1    |\n",
      "|Dataset                     |2    |\n",
      "|return                      |2    |\n",
      "|from                        |1    |\n",
      "|containing                  |1    |\n",
      "|temporary                   |1    |\n",
      "|Limit                       |1    |\n",
      "|make                        |1    |\n",
      "|work                        |1    |\n",
      "|methods                     |1    |\n",
      "|single                      |1    |\n",
      "|and                         |4    |\n",
      "|it.                         |1    |\n",
      "|operations                  |2    |\n",
      "|are                         |8    |\n",
      "|SQL                         |1    |\n",
      "|Datasets                    |3    |\n",
      "|of                          |2    |\n",
      "|Datasets.                   |4    |\n",
      "|results,                    |1    |\n",
      "|console                     |1    |\n",
      "|yet                         |1    |\n",
      "|an                          |1    |\n",
      "|###                         |1    |\n",
      "|that                        |2    |\n",
      "|running                     |1    |\n",
      "|run                         |1    |\n",
      "|all                         |1    |\n",
      "|a                           |11   |\n",
      "|as                          |1    |\n",
      "|register                    |2    |\n",
      "|                            |48   |\n",
      "|show()                      |1    |\n",
      "+----------------------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = writer.start() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fa2f43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7258599",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 11:53:13 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6d6c318d-9e12-4510-9a8a-eb2366a9ce64. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/01 11:53:13 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|Words|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----------------------------------+-----+\n",
      "|Words                              |count|\n",
      "+-----------------------------------+-----+\n",
      "|few                                |3    |\n",
      "|those                              |1    |\n",
      "|some                               |1    |\n",
      "|recognize                          |1    |\n",
      "|input                              |9    |\n",
      "|operation                          |2    |\n",
      "|import                             |9    |\n",
      "|developers,                        |1    |\n",
      "|received                           |2    |\n",
      "|column                             |2    |\n",
      "|map                                |1    |\n",
      "|latencies</b>                      |1    |\n",
      "|achieve                            |1    |\n",
      "|applications)                      |1    |\n",
      "|place,                             |1    |\n",
      "|If                                 |7    |\n",
      "|API                                |3    |\n",
      "|you.                               |1    |\n",
      "|fewcounts.printSchema()            |1    |\n",
      "|recommends                         |1    |\n",
      "|thread                             |5    |\n",
      "|Instead,                           |1    |\n",
      "|operators.                         |1    |\n",
      "|joins                              |1    |\n",
      "|sense                              |1    |\n",
      "|used                               |6    |\n",
      "|(e.g.                              |1    |\n",
      "|seconds)                           |2    |\n",
      "|writer2_words                      |1    |\n",
      "|programmatic                       |1    |\n",
      "|newline-separated                  |1    |\n",
      "|production                         |1    |\n",
      "|whether                            |1    |\n",
      "|earlier                            |1    |\n",
      "|interval.                          |2    |\n",
      "|Philosophy                         |1    |\n",
      "|scale                              |1    |\n",
      "|us                                 |1    |\n",
      "|counter                            |1    |\n",
      "|returns                            |2    |\n",
      "|two                                |4    |\n",
      "|Five                               |1    |\n",
      "|metadata                           |1    |\n",
      "|classes                            |1    |\n",
      "|lack                               |1    |\n",
      "|complex                            |1    |\n",
      "|<b>Continuous:                     |1    |\n",
      "|Default                            |1    |\n",
      "|dealing                            |1    |\n",
      "|experimental                       |1    |\n",
      "|micro-batch.                       |1    |\n",
      "|daemon                             |3    |\n",
      "|server                             |1    |\n",
      "|color='red'>In                     |1    |\n",
      "|within                             |1    |\n",
      "|streamingQuery_lines.stop()        |2    |\n",
      "|at-most-once                       |1    |\n",
      "|=                                  |61   |\n",
      "|aggregations                       |3    |\n",
      "|![image.png](attachment:image.png) |2    |\n",
      "|progress                           |1    |\n",
      "|set                                |2    |\n",
      "|always                             |1    |\n",
      "|ensures                            |2    |\n",
      "|regardless                         |1    |\n",
      "|discovery                          |1    |\n",
      "|table,                             |4    |\n",
      "|own.                               |1    |\n",
      "|cheapest—highest                   |1    |\n",
      "|Processing                         |1    |\n",
      "|developer                          |2    |\n",
      "|lines.select(split(col(\"value\"),   |4    |\n",
      "|reading                            |5    |\n",
      "|name                               |1    |\n",
      "|computed                           |1    |\n",
      "|Fundamentals                       |1    |\n",
      "|\"\\\\s\").alias(\"word\"))              |4    |\n",
      "|starting                           |2    |\n",
      "|words.groupBy(\"word\").count()      |6    |\n",
      "|down                               |1    |\n",
      "|Thinking                           |1    |\n",
      "|generates                          |1    |\n",
      "|that,                              |1    |\n",
      "|data,                              |2    |\n",
      "|gradual                            |1    |\n",
      "|stream                             |10   |\n",
      "|not                                |17   |\n",
      "|low                                |4    |\n",
      "|fault-tolerance                    |2    |\n",
      "|<b>One-time                        |1    |\n",
      "|saves                              |1    |\n",
      "|Internally,                        |1    |\n",
      "|Distinct                           |1    |\n",
      "|streamingQuery.awaitTermination(20)|1    |\n",
      "|deprecated                         |1    |\n",
      "|will                               |28   |\n",
      "|monitoring                         |1    |\n",
      "|trigger(processingTime):           |1    |\n",
      "|applied                            |3    |\n",
      "|(that                              |1    |\n",
      "+-----------------------------------+-----+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q= writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "078120b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "92a12520",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 12:04:14 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-ca9ff29a-eb77-4fbf-8e53-8b6da93e32d2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/01 12:04:14 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----+-----+\n",
      "|Words|count|\n",
      "+-----+-----+\n",
      "+-----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+---------------------------------------+-----+\n",
      "|Words                                  |count|\n",
      "+---------------------------------------+-----+\n",
      "|import                                 |1    |\n",
      "|=                                      |6    |\n",
      "|lines.select(split(col(\"value\"),       |1    |\n",
      "|\"\\\\s\").alias(\"word\"))                  |1    |\n",
      "|words.groupBy(\"word\").count()          |1    |\n",
      "|\")).alias(\"word\"))                     |1    |\n",
      "|counts                                 |1    |\n",
      "|#words                                 |1    |\n",
      "|streamingQuery.stop()                  |2    |\n",
      "|*                                      |1    |\n",
      "|.option(\"port\",                        |1    |\n",
      "|words                                  |1    |\n",
      "|(spark                                 |1    |\n",
      "|.load())                               |1    |\n",
      "|lines                                  |1    |\n",
      "|.outputMode(\"complete\")                |1    |\n",
      "|.format(\"console\")                     |1    |\n",
      "|12345)                                 |1    |\n",
      "|checkpointDir)                         |2    |\n",
      "|\"localhost\")                           |1    |\n",
      "|\"chkpnt2\"                              |1    |\n",
      "|streamingQuery                         |1    |\n",
      "|.option(\"checkpointLocation\",          |2    |\n",
      "|from                                   |1    |\n",
      "|lines.select(explode(split(lines.value,|1    |\n",
      "|.readStream.format(\"socket\")           |1    |\n",
      "|\"                                      |1    |\n",
      "|pyspark.sql.functions                  |1    |\n",
      "|(counts.writeStream                    |1    |\n",
      "|.start())                              |2    |\n",
      "|                                       |148  |\n",
      "|.option(\"host\",                        |1    |\n",
      "|checkpointDir                          |1    |\n",
      "+---------------------------------------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 2\n",
      "-------------------------------------------\n",
      "+---------------------------------------+-----+\n",
      "|Words                                  |count|\n",
      "+---------------------------------------+-----+\n",
      "|import                                 |1    |\n",
      "|=                                      |6    |\n",
      "|lines.select(split(col(\"value\"),       |1    |\n",
      "|\"\\\\s\").alias(\"word\"))                  |1    |\n",
      "|words.groupBy(\"word\").count()          |1    |\n",
      "|\")).alias(\"word\"))                     |1    |\n",
      "|counts                                 |1    |\n",
      "|you                                    |1    |\n",
      "|#words                                 |1    |\n",
      "|streamingQuery.stop()                  |2    |\n",
      "|*                                      |1    |\n",
      "|how                                    |1    |\n",
      "|.option(\"port\",                        |1    |\n",
      "|words                                  |1    |\n",
      "|(spark                                 |1    |\n",
      "|.load())                               |1    |\n",
      "|lines                                  |1    |\n",
      "|.outputMode(\"complete\")                |1    |\n",
      "|.format(\"console\")                     |1    |\n",
      "|12345)                                 |1    |\n",
      "|checkpointDir)                         |2    |\n",
      "|\"localhost\")                           |1    |\n",
      "|\"chkpnt2\"                              |1    |\n",
      "|streamingQuery                         |1    |\n",
      "|.option(\"checkpointLocation\",          |2    |\n",
      "|from                                   |1    |\n",
      "|are                                    |1    |\n",
      "|lines.select(explode(split(lines.value,|1    |\n",
      "|.readStream.format(\"socket\")           |1    |\n",
      "|\"                                      |1    |\n",
      "|pyspark.sql.functions                  |1    |\n",
      "|(counts.writeStream                    |1    |\n",
      "|hi                                     |1    |\n",
      "|.start())                              |2    |\n",
      "|                                       |148  |\n",
      "|.option(\"host\",                        |1    |\n",
      "|checkpointDir                          |1    |\n",
      "+---------------------------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3a4f4267",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cdf2beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'date STRING, delay INTEGER, distance INTEGER, origin STRING, destination STRING'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a265fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format('csv')\\\n",
    ".option('header','true')\\\n",
    ".schema(s)\\\n",
    ".load('MyInputStream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96e99b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format('parquet')\\\n",
    ".schema(s)\\\n",
    ".load('MyInputStream')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30391ff7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f369491a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- delay: integer (nullable = true)\n",
      " |-- distance: integer (nullable = true)\n",
      " |-- origin: string (nullable = true)\n",
      " |-- destination: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d605577",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20162615",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.groupBy('date').agg(avg('delay').alias('AvgDelay'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0fa7854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0e69ac0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: string (nullable = true)\n",
      " |-- AvgDelay: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e47dff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df2.writeStream.format('console')\\\n",
    ".outputMode('complete')\\\n",
    ".option('truncate','false')\\\n",
    ".option('numrows',100)\\\n",
    ".trigger(processingTime='20 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b7618bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 12:42:27 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a3c6889d-0400-4c1a-aa11-3591e32b89e6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/01 12:42:27 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+--------+\n",
      "|date   |AvgDelay|\n",
      "+-------+--------+\n",
      "|1011245|6.0     |\n",
      "|1020605|-4.0    |\n",
      "|1061230|0.0     |\n",
      "|1021245|-2.0    |\n",
      "|1061215|-6.0    |\n",
      "|1030605|0.0     |\n",
      "|1020600|-8.0    |\n",
      "|1071230|0.0     |\n",
      "|1041243|10.0    |\n",
      "|1051245|88.0    |\n",
      "|1040605|28.0    |\n",
      "|1060625|-3.0    |\n",
      "|1070600|0.0     |\n",
      "|1031245|-4.0    |\n",
      "|1061725|69.0    |\n",
      "|1050605|9.0     |\n",
      "|1071725|0.0     |\n",
      "|1070625|0.0     |\n",
      "|1071219|0.0     |\n",
      "+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+--------+\n",
      "|date   |AvgDelay|\n",
      "+-------+--------+\n",
      "|1011245|6.0     |\n",
      "|1020605|-4.0    |\n",
      "|1121215|-5.0    |\n",
      "|1140625|-5.0    |\n",
      "|1061230|0.0     |\n",
      "|1021245|-2.0    |\n",
      "|1061215|-6.0    |\n",
      "|1121725|-1.0    |\n",
      "|1030605|0.0     |\n",
      "|1131230|-13.0   |\n",
      "|1020600|-8.0    |\n",
      "|1151219|0.0     |\n",
      "|1150625|0.0     |\n",
      "|1131215|14.0    |\n",
      "|1131725|-6.0    |\n",
      "|1071230|0.0     |\n",
      "|1041243|10.0    |\n",
      "|1131219|-8.0    |\n",
      "|1051245|88.0    |\n",
      "|1141230|-8.0    |\n",
      "|1040605|28.0    |\n",
      "|1151725|-6.0    |\n",
      "|1130625|29.0    |\n",
      "|1060625|-3.0    |\n",
      "|1070600|0.0     |\n",
      "|1151230|0.0     |\n",
      "|1130600|-7.0    |\n",
      "|1031245|-4.0    |\n",
      "|1061725|69.0    |\n",
      "|1050605|9.0     |\n",
      "|1141725|-9.0    |\n",
      "|1140600|-9.0    |\n",
      "|1071725|0.0     |\n",
      "|1      |2.0     |\n",
      "|1070625|0.0     |\n",
      "|1071219|0.0     |\n",
      "|1150607|0.0     |\n",
      "|1150600|0.0     |\n",
      "|1141219|-10.0   |\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q=writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "315659a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 12:48:00 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-2da1f4a7-c449-4b4d-9f43-f2fa347233b6. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/01 12:48:00 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+--------+\n",
      "|date   |AvgDelay|\n",
      "+-------+--------+\n",
      "|1011245|6.0     |\n",
      "|1020605|-4.0    |\n",
      "|1121215|-5.0    |\n",
      "|1140625|-5.0    |\n",
      "|1061230|0.0     |\n",
      "|1021245|-2.0    |\n",
      "|1061215|-6.0    |\n",
      "|1121725|-1.0    |\n",
      "|1030605|0.0     |\n",
      "|1131230|-13.0   |\n",
      "|1020600|-8.0    |\n",
      "|1151219|0.0     |\n",
      "|1131215|14.0    |\n",
      "|1150625|0.0     |\n",
      "|1131725|-6.0    |\n",
      "|1071230|0.0     |\n",
      "|1041243|10.0    |\n",
      "|1131219|-8.0    |\n",
      "|1051245|88.0    |\n",
      "|1141230|-8.0    |\n",
      "|1040605|28.0    |\n",
      "|1151725|-6.0    |\n",
      "|1130625|29.0    |\n",
      "|1060625|-3.0    |\n",
      "|1070600|0.0     |\n",
      "|1151230|0.0     |\n",
      "|1130600|-7.0    |\n",
      "|1031245|-4.0    |\n",
      "|1061725|69.0    |\n",
      "|1050605|9.0     |\n",
      "|1141725|-9.0    |\n",
      "|1140600|-9.0    |\n",
      "|1071725|0.0     |\n",
      "|1      |2.0     |\n",
      "|1070625|0.0     |\n",
      "|1071219|0.0     |\n",
      "|1150607|0.0     |\n",
      "|1150600|0.0     |\n",
      "|1141219|-10.0   |\n",
      "+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 12:48:27 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 20000 milliseconds, but spent 25779 milliseconds\n"
     ]
    }
   ],
   "source": [
    "q=writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32a9fd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "584bfe07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 12:51:36 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-88490d25-4386-492e-aeeb-ea203443081d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/01 12:51:36 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------+--------+\n",
      "|date   |AvgDelay|\n",
      "+-------+--------+\n",
      "|1110600|-9.0    |\n",
      "|1110625|-4.0    |\n",
      "|1090600|151.0   |\n",
      "|1100600|-5.0    |\n",
      "|1080607|5.0     |\n",
      "|1081219|54.0    |\n",
      "|1111215|127.0   |\n",
      "|1091725|0.0     |\n",
      "|1091230|-4.0    |\n",
      "|1090625|8.0     |\n",
      "|1101725|7.0     |\n",
      "|1091215|43.0    |\n",
      "|1100625|52.0    |\n",
      "|1080625|1.0     |\n",
      "|1091219|83.0    |\n",
      "|1101219|0.0     |\n",
      "|1101215|-5.0    |\n",
      "|1101230|-8.0    |\n",
      "|1081230|33.0    |\n",
      "|1080600|0.0     |\n",
      "+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-------+--------+\n",
      "|date   |AvgDelay|\n",
      "+-------+--------+\n",
      "|1011245|6.0     |\n",
      "|1020605|-4.0    |\n",
      "|1110600|-9.0    |\n",
      "|1110625|-4.0    |\n",
      "|1061230|0.0     |\n",
      "|1090600|151.0   |\n",
      "|1100600|-5.0    |\n",
      "|1021245|-2.0    |\n",
      "|1061215|-6.0    |\n",
      "|1080607|5.0     |\n",
      "|1030605|0.0     |\n",
      "|1020600|-8.0    |\n",
      "|1081219|54.0    |\n",
      "|1111215|127.0   |\n",
      "|1071230|0.0     |\n",
      "|1041243|10.0    |\n",
      "|1051245|88.0    |\n",
      "|1091725|0.0     |\n",
      "|1091230|-4.0    |\n",
      "|1040605|28.0    |\n",
      "|1090625|8.0     |\n",
      "|1101725|7.0     |\n",
      "|1060625|-3.0    |\n",
      "|1070600|0.0     |\n",
      "|1091215|43.0    |\n",
      "|1031245|-4.0    |\n",
      "|1061725|69.0    |\n",
      "|1050605|9.0     |\n",
      "|1100625|52.0    |\n",
      "|1080625|1.0     |\n",
      "|1091219|83.0    |\n",
      "|1071725|0.0     |\n",
      "|1101219|0.0     |\n",
      "|1101215|-5.0    |\n",
      "|1101230|-8.0    |\n",
      "|1070625|0.0     |\n",
      "|1071219|0.0     |\n",
      "|1081230|33.0    |\n",
      "|1080600|0.0     |\n",
      "+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 12:53:00 ERROR Executor: Exception in task 0.0 in stage 8.0 (TID 807)\n",
      "java.lang.RuntimeException: file:/home/hatem/PySpark/Intake 44/L5_StructuredStreaming/MyInputStream/D1.csv is not a Parquet file. Expected magic number at tail, but found [79, 82, 68, 10]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:213)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "24/06/01 12:53:00 WARN TaskSetManager: Lost task 0.0 in stage 8.0 (TID 807) (10.0.2.15 executor driver): java.lang.RuntimeException: file:/home/hatem/PySpark/Intake 44/L5_StructuredStreaming/MyInputStream/D1.csv is not a Parquet file. Expected magic number at tail, but found [79, 82, 68, 10]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:213)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "24/06/01 12:53:00 ERROR TaskSetManager: Task 0 in stage 8.0 failed 1 times; aborting job\n",
      "24/06/01 12:53:00 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=100, truncate=false]] is aborting.\n",
      "24/06/01 12:53:00 ERROR WriteToDataSourceV2Exec: Data source write support MicroBatchWrite[epoch: 2, writer: ConsoleWriter[numRows=100, truncate=false]] aborted.\n",
      "24/06/01 12:53:00 ERROR MicroBatchExecution: Query [id = 42c7fda6-b049-45a7-8e7f-c19627c667e9, runId = d26cd90c-9e9e-442f-9192-ec4096285f02] terminated with error\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 8.0 failed 1 times, most recent failure: Lost task 0.0 in stage 8.0 (TID 807) (10.0.2.15 executor driver): java.lang.RuntimeException: file:/home/hatem/PySpark/Intake 44/L5_StructuredStreaming/MyInputStream/D1.csv is not a Parquet file. Expected magic number at tail, but found [79, 82, 68, 10]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:213)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2844)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2780)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2779)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2779)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1242)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1242)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3048)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2982)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2971)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:984)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2(WriteToDataSourceV2Exec.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2TableWriteExec.writeWithV2$(WriteToDataSourceV2Exec.scala:359)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.writeWithV2(WriteToDataSourceV2Exec.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.WriteToDataSourceV2Exec.run(WriteToDataSourceV2Exec.scala:318)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result$lzycompute(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.result(V2CommandExec.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.v2.V2CommandExec.executeCollect(V2CommandExec.scala:49)\n",
      "\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4344)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$collect$1(Dataset.scala:3585)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4334)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n",
      "\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4332)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4332)\n",
      "\tat org.apache.spark.sql.Dataset.collect(Dataset.scala:3585)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$17(MicroBatchExecution.scala:741)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runBatch$16(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runBatch(MicroBatchExecution.scala:729)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$2(MicroBatchExecution.scala:286)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken(ProgressReporter.scala:427)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProgressReporter.reportTimeTaken$(ProgressReporter.scala:425)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.reportTimeTaken(StreamExecution.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.$anonfun$runActivatedStream$1(MicroBatchExecution.scala:249)\n",
      "\tat org.apache.spark.sql.execution.streaming.ProcessingTimeExecutor.execute(TriggerExecutor.scala:67)\n",
      "\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.runActivatedStream(MicroBatchExecution.scala:239)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:311)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:289)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$1(StreamExecution.scala:211)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n",
      "\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:211)\n",
      "Caused by: java.lang.RuntimeException: file:/home/hatem/PySpark/Intake 44/L5_StructuredStreaming/MyInputStream/D1.csv is not a Parquet file. Expected magic number at tail, but found [79, 82, 68, 10]\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.readFooter(ParquetFileReader.java:565)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.<init>(ParquetFileReader.java:799)\n",
      "\tat org.apache.parquet.hadoop.ParquetFileReader.open(ParquetFileReader.java:666)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:85)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:71)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFooterReader.readFooter(ParquetFooterReader.java:66)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat.$anonfun$buildReaderWithPartitionValues$2(ParquetFileFormat.scala:213)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.org$apache$spark$sql$execution$datasources$FileScanRDD$$anon$$readCurrentFile(FileScanRDD.scala:217)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:279)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.hashAgg_doAggregateWithKeys_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n"
     ]
    }
   ],
   "source": [
    "q=writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c391f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.format('parquet')\\\n",
    ".outputMode('append')\\\n",
    ".option('path','Outstream')\\\n",
    ".option('checkpointLocation','chkpointMans')\\\n",
    ".trigger(processingTime='2 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41edd4fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 13:00:39 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    }
   ],
   "source": [
    "q = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a2578018",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c72c806",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df2.writeStream.format('memory')\\\n",
    ".outputMode('complete')\\\n",
    ".queryName('myQuery')\\\n",
    ".trigger(processingTime='2 seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "041576c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 13:06:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-3497c455-dd1d-4b43-abff-f9f58b9795a5. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/06/01 13:06:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/06/01 13:07:20 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 23791 milliseconds\n"
     ]
    }
   ],
   "source": [
    "q = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "880b4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q = spark.sql('select * from myQuery')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bd201da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_q.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4db9f37a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|   date|AvgDelay|\n",
      "+-------+--------+\n",
      "|1011245|     6.0|\n",
      "|1020605|    -4.0|\n",
      "|1110600|    -9.0|\n",
      "|1110625|    -4.0|\n",
      "|1061230|     0.0|\n",
      "|1090600|   151.0|\n",
      "|1100600|    -5.0|\n",
      "|1021245|    -2.0|\n",
      "|1061215|    -6.0|\n",
      "|1080607|     5.0|\n",
      "|1030605|     0.0|\n",
      "|1020600|    -8.0|\n",
      "|1081219|    54.0|\n",
      "|1111215|   127.0|\n",
      "|1071230|     0.0|\n",
      "|1041243|    10.0|\n",
      "|1051245|    88.0|\n",
      "|1091725|     0.0|\n",
      "|1091230|    -4.0|\n",
      "|1040605|    28.0|\n",
      "|1090625|     8.0|\n",
      "|1101725|     7.0|\n",
      "|1060625|    -3.0|\n",
      "|1070600|     0.0|\n",
      "|1091215|    43.0|\n",
      "|1031245|    -4.0|\n",
      "|1061725|    69.0|\n",
      "|1050605|     9.0|\n",
      "|1100625|    52.0|\n",
      "|1080625|     1.0|\n",
      "|1091219|    83.0|\n",
      "|1071725|     0.0|\n",
      "|1101219|     0.0|\n",
      "|1101215|    -5.0|\n",
      "|1101230|    -8.0|\n",
      "|1070625|     0.0|\n",
      "|1071219|     0.0|\n",
      "|1081230|    33.0|\n",
      "|1080600|     0.0|\n",
      "+-------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/06/01 13:08:17 WARN ProcessingTimeExecutor: Current batch is falling behind. The trigger interval is 2000 milliseconds, but spent 15165 milliseconds\n"
     ]
    }
   ],
   "source": [
    "df_q.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "843c277d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------+\n",
      "|   date|AvgDelay|\n",
      "+-------+--------+\n",
      "|1011245|     6.0|\n",
      "|1020605|    -4.0|\n",
      "|1121215|    -5.0|\n",
      "|1140625|    -5.0|\n",
      "|1110600|    -9.0|\n",
      "|1110625|    -4.0|\n",
      "|1061230|     0.0|\n",
      "|1090600|   151.0|\n",
      "|1100600|    -5.0|\n",
      "|1021245|    -2.0|\n",
      "|1061215|    -6.0|\n",
      "|1080607|     5.0|\n",
      "|1121725|    -1.0|\n",
      "|1030605|     0.0|\n",
      "|1131230|   -13.0|\n",
      "|1020600|    -8.0|\n",
      "|1151219|     0.0|\n",
      "|1081219|    54.0|\n",
      "|1150625|     0.0|\n",
      "|1131215|    14.0|\n",
      "|1111215|   127.0|\n",
      "|1131725|    -6.0|\n",
      "|1071230|     0.0|\n",
      "|1041243|    10.0|\n",
      "|1131219|    -8.0|\n",
      "|1051245|    88.0|\n",
      "|1091725|     0.0|\n",
      "|1091230|    -4.0|\n",
      "|1141230|    -8.0|\n",
      "|1040605|    28.0|\n",
      "|1151725|    -6.0|\n",
      "|1090625|     8.0|\n",
      "|1130625|    29.0|\n",
      "|1101725|     7.0|\n",
      "|1060625|    -3.0|\n",
      "|1070600|     0.0|\n",
      "|1091215|    43.0|\n",
      "|1151230|     0.0|\n",
      "|1130600|    -7.0|\n",
      "|1031245|    -4.0|\n",
      "|1061725|    69.0|\n",
      "|1050605|     9.0|\n",
      "|1100625|    52.0|\n",
      "|1141725|    -9.0|\n",
      "|1080625|     1.0|\n",
      "|1140600|    -9.0|\n",
      "|1091219|    83.0|\n",
      "|1071725|     0.0|\n",
      "|1101219|     0.0|\n",
      "|1101215|    -5.0|\n",
      "|1101230|    -8.0|\n",
      "|1070625|     0.0|\n",
      "|1071219|     0.0|\n",
      "|1081230|    33.0|\n",
      "|1080600|     0.0|\n",
      "|1150607|     0.0|\n",
      "|1150600|     0.0|\n",
      "|1141219|   -10.0|\n",
      "+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_q = spark.sql('select * from myQuery')\n",
    "df_q.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ebd6cdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "q.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115f58e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
