{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8074fc",
   "metadata": {},
   "source": [
    "## Streaming Data Sources and Sinks\n",
    "- You can create DataFrames from streaming sources using\n",
    "<b>SparkSession.readStream()</b> and write the output from a result DataFrame using\n",
    "<b>DataFrame.writeStream()</b>.\n",
    "- In each case, you can specify the source type using the\n",
    "method <b>format()</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4405489",
   "metadata": {},
   "source": [
    "## Files\n",
    "- Structured Streaming supports reading and writing data streams to and from files in\n",
    "the same formats as the ones supported in batch processing: <b>plain text, CSV, JSON,\n",
    "Parquet, ORC, etc.</b>\n",
    "\n",
    "### Reading from files\n",
    "- Structured Streaming can treat files written into a directory as a data stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07cf3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Read lines from a file stream\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f746304a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDirectory = 'MyInputStream/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4c7040",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.readStream.format(\"text\") \\\n",
    "    .load('MyInputStream/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dafb6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = df.writeStream.outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .option(\"numRows\", 100)\n",
    "#     .option('checkpointLocation','chkpnt')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "69b183ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/07 14:32:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-1bec4a0e-e686-45ad-a435-3029abf163fc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/07 14:32:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+--------------------------------------+\n",
      "|value                                 |\n",
      "+--------------------------------------+\n",
      "|date,delay,distance,origin,destination|\n",
      "|1121215,-5,602,ABE,ATL                |\n",
      "|1121725,-1,602,ABE,ATL                |\n",
      "|1131215,14,602,ABE,ATL                |\n",
      "|1130600,-7,369,ABE,DTW                |\n",
      "|1131725,-6,602,ABE,ATL                |\n",
      "|1131230,-13,369,ABE,DTW               |\n",
      "|1130625,29,602,ABE,ATL                |\n",
      "|1131219,-8,569,ABE,ORD                |\n",
      "|1140600,-9,369,ABE,DTW                |\n",
      "|1141725,-9,602,ABE,ATL                |\n",
      "|1141230,-8,369,ABE,DTW                |\n",
      "|1140625,-5,602,ABE,ATL                |\n",
      "|1141219,-10,569,ABE,ORD               |\n",
      "|1150600,0,369,ABE,DTW                 |\n",
      "|1151725,-6,602,ABE,ATL                |\n",
      "|1151230,0,369,ABE,DTW                 |\n",
      "|1150625,0,602,ABE,ATL                 |\n",
      "|1150607,0,569,ABE,ORD                 |\n",
      "|1151219,0,569,ABE,ORD                 |\n",
      "|44,44,44,44,44                        |\n",
      "|date,delay,distance,origin,destination|\n",
      "|1011245,6,602,ABE,ATL                 |\n",
      "|1020600,-8,369,ABE,DTW                |\n",
      "|1021245,-2,602,ABE,ATL                |\n",
      "|1020605,-4,602,ABE,ATL                |\n",
      "|1031245,-4,602,ABE,ATL                |\n",
      "|1030605,0,602,ABE,ATL                 |\n",
      "|1041243,10,602,ABE,ATL                |\n",
      "|1040605,28,602,ABE,ATL                |\n",
      "|1051245,88,602,ABE,ATL                |\n",
      "|1050605,9,602,ABE,ATL                 |\n",
      "|1061215,-6,602,ABE,ATL                |\n",
      "|1061725,69,602,ABE,ATL                |\n",
      "|1061230,0,369,ABE,DTW                 |\n",
      "|1060625,-3,602,ABE,ATL                |\n",
      "|1070600,0,369,ABE,DTW                 |\n",
      "|1071725,0,602,ABE,ATL                 |\n",
      "|1071230,0,369,ABE,DTW                 |\n",
      "|1070625,0,602,ABE,ATL                 |\n",
      "|1071219,0,569,ABE,ORD                 |\n",
      "|11,11,11,11,11                        |\n",
      "+--------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/07 14:33:05 WARN HadoopFSUtils: The directory file:/home/hatem/PySpark/Ubuntu_Final_Spark_Intake_43/L5_StructuredStreaming/MyInputStream/lu530236y701.tmp was not found. Was it deleted very recently?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 1\n",
      "-------------------------------------------\n",
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = writer.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c96df35e",
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f67c5c1",
   "metadata": {},
   "source": [
    "- The returned streaming DataFrame will have the specified schema. - Here are a few key points to remember when using files:\n",
    "    - All the files must be of the same format and are expected to have the same schema. For example, if the format is \"json\" , all the files must be in the JSON format with one JSON record per line. The schema of each JSON record must match the one specified with readStream() . Violation of these assumptions can lead to incorrect parsing (e.g., unexpected null values) or query failures.\n",
    "    - Each file must appear in the directory listing atomicallyâ€”that is, the whole file must be available at once for reading, and once it is available, the file cannot be updated or modified. This is because Structured Streaming will process the file when the engine finds it (using directory listing) and internally mark it as processed. Any changes to that file will not be processed.\n",
    "    - When there are multiple new files to process but it can only pick some of them in the next micro-batch (e.g., because of rate limits), it will select the files with the earliest timestamps. Within the micro-batch, however, there is no predefined order of reading of the selected files; all of them will be read in parallel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25c68ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
